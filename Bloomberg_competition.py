# -*- coding: utf-8 -*-
"""CITIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QzlbPPhcjhBsixDlglpobbwihwDcWqiP

## **Import Libraries**
"""

import pandas as pd
from pandas.plotting import autocorrelation_plot
from pandas import DataFrame
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.api import VAR
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
from xgboost import XGBRegressor, plot_importance
from sklearn.metrics import mean_squared_error

"""## **Macro Factors**

### **US Data Extraction**

#### ADP
"""

# ADP 非农就业
df_non_agri = pd.read_excel('非农就业人数(月).xls', names=['Date','ADP non-agri'])  
df_non_agri

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_non_agri.shape[0]), np.array(df_non_agri.iloc[:,1]),label='ADP non-agri')
plt.legend()
plt.show()

"""#### US 企业债收益率"""

df_Cor_bond = pd.read_excel('US企业债收益率(月).xls', names=['Date','US_Bond_return'])  
df_Cor_bond = df_Cor_bond.iloc[191:-1,:].reset_index().drop(["index"], axis = 1)
df_Cor_bond

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_Cor_bond.shape[0]), np.array(df_Cor_bond.iloc[:,1]),label='US Corporate Bond Return')
plt.legend()
plt.show()

"""#### US 储备资产"""

df_gov_asset = pd.read_excel('US储备资产(月).xls', names=['Date','US_gov_assets'])  
df_gov_asset = df_gov_asset.iloc[14:,:].reset_index().drop(["index"], axis = 1)
df_gov_asset

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_gov_asset.shape[0]), np.array(df_gov_asset.iloc[:,1]),label='US Government Reserve Asset')
plt.legend()
plt.show()

"""#### US 基准利率"""

dates = df_gov_asset.Date.tolist()
df_us_base_rate = pd.read_excel('US基准利率.xls', names=['Date','US_base_rate'])  
df_us_base_rate['monthly_base_rate'] = df_us_base_rate['US_base_rate'].rolling(27, min_periods = 1).mean() 
df_us_base_rate = df_us_base_rate.iloc[::27,:]
df_us_base_rate = df_us_base_rate.iloc[15:-3,:].reset_index().drop(["index"],axis=1)
df_us_base_rate = df_us_base_rate.drop(["US_base_rate"],axis=1)
df_us_base_rate["Date"] = np.array(dates)
df_us_base_rate

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_base_rate.shape[0]), np.array(df_us_base_rate.iloc[:,1]),label='US Base Rate')
plt.legend()
plt.show()

"""#### 外汇储备"""

df_us_forei_res = pd.read_excel('US外汇储备(月).xls', names=['Date','US_foreign_asset_rev'])  
df_us_forei_res = df_us_forei_res.iloc[14:,:].reset_index().drop(["index"], axis = 1)
df_us_forei_res

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_forei_res.shape[0]), np.array(df_us_forei_res.iloc[:,1]),label='US Foreign Reserve')
plt.legend()
plt.show()

"""#### US VIX"""

dates = df_gov_asset.Date.tolist()
df_vix = pd.read_excel('US市场波动率指数(VIX).xls', names=['Date','VIX'])  
df_vix = df_vix.iloc[2524:,:].reset_index().drop(["index"], axis = 1)
df_vix['monthly_vix'] = df_vix['VIX'].rolling(21, min_periods = 1).mean() 
df_vix = df_vix.iloc[::21,:].reset_index().drop(["index"],axis=1)
df_vix = df_vix.iloc[:-2,:].drop(["VIX"],axis=1)
df_vix["Date"] = np.array(dates)
df_vix

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_vix.shape[0]), np.array(df_vix.iloc[:,1]),label='US VIX')
plt.legend()
plt.show()

"""#### US 消费信贷"""

df_us_com_cre = pd.read_excel('US消费信贷(月).xls', names=['Date','US_consumer_credit'])  
df_us_com_cre = df_us_com_cre.iloc[683:,:].reset_index().drop(["index"], axis = 1)
df_us_com_cre

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_com_cre.shape[0]), np.array(df_us_com_cre.iloc[:,1]),label='US Consumer Credit')
plt.legend()
plt.show()

"""#### US CPI"""

df_us_cpi = pd.read_excel('US消费者物价指数(CPI)同比(月).xls', names=['Date','US_CPI'])  
df_us_cpi = df_us_cpi.iloc[1031:-1,:].reset_index().drop(["index"], axis = 1)
df_us_cpi

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_cpi.shape[0]), np.array(df_us_cpi.iloc[:,1]),label='US CPI')
plt.legend()
plt.show()

"""#### US政府赤字"""

df_us_deficit = pd.read_excel('US联邦政府财政赤字(月).xls', names=['Date','US_gov_deficit'])  
df_us_deficit = df_us_deficit.iloc[230:-1,:].reset_index().drop(["index"], axis = 1)
df_us_deficit

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_deficit.shape[0]), np.array(df_us_deficit.iloc[:,1]),label='US Deficit')
plt.legend()
plt.show()

"""#### US货币供应量"""

df_us_money_supply = pd.read_excel('US货币供应量(月).xls', names=['Date','US_money_supply'])  
df_us_money_supply = df_us_money_supply.iloc[491:-1,:].reset_index().drop(["index"], axis = 1)
df_us_money_supply

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_money_supply.shape[0]), np.array(df_us_money_supply.iloc[:,1]),label='US Money Supply')
plt.legend()
plt.show()

"""#### US 进口价格指数"""

df_us_import_price_index = pd.read_excel('US进口价格指数(月).xls', names=['Date','import_price_index'])  
df_us_import_price_index = df_us_import_price_index.iloc[157:-1,:].reset_index().drop(["index"], axis = 1)
df_us_import_price_index

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_import_price_index.shape[0]), 
         np.array(df_us_import_price_index.iloc[:,1]),label='Import Price Index')
plt.legend()
plt.show()

"""#### US PMI"""

df_us_pmi = pd.read_excel('US采购经理指数(PMI)(月).xls', names=['Date','US_pmi'])  
df_us_pmi = df_us_pmi.iloc[623:-1,:].reset_index().drop(["index"], axis = 1)
df_us_pmi

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_us_pmi.shape[0]), 
         np.array(df_us_pmi.iloc[:,1]),label='US pmi')
plt.legend()
plt.show()

"""#### Combine all date"""

df_US = pd.concat([df_us_money_supply,df_Cor_bond, df_gov_asset,df_us_base_rate,
                   df_us_forei_res,df_vix,df_us_com_cre,df_us_cpi,df_us_deficit,
                   df_us_pmi,df_us_import_price_index, df_non_agri], 
                  axis=1).drop(["Date"], axis =1) 
df_US.insert(loc=0, column='Date', value=dates)
df_US

"""### **China Data Extraction**

#### M0 供应量
"""

# ADP 就业数据
df_M0 = pd.read_excel('M0供应量(月).xls', names=['Date','M0'])  
df_M0 = df_M0[65:-1].reset_index().drop(["index"], axis = 1)
df_M0

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_M0.shape[0]), 
         np.array(df_M0.iloc[:,1]),label='M0')
plt.legend()
plt.show()

"""#### 中国CPI"""

df_Chinese_CPI = pd.read_excel('中国CPI当月同比(月).xls', names=['Date','Chinese CPI'])  
df_Chinese_CPI = df_Chinese_CPI.iloc[155:-1,:].reset_index().drop(["index"], axis = 1)
df_Chinese_CPI

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_Chinese_CPI.shape[0]), 
         np.array(df_Chinese_CPI.iloc[:,1]),label='China CPI')
plt.legend()
plt.show()

"""#### 中国投资海外证券情况"""

df_oversea_inv = pd.read_excel('中国投资海外证券情况(月).xls', names=['Date','CHN_oversea_sec_inv'])  
df_oversea_inv = df_oversea_inv.iloc[252:,:].reset_index().drop(["index"], axis = 1)
df_oversea_inv

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_Chinese_CPI.shape[0]), 
         np.array(df_Chinese_CPI.iloc[:,1]),label='China CPI')
plt.legend()
plt.show()

"""#### 当月规模以上工业增加值"""

df_industry_increment = pd.read_excel('规模以上工业增加值当月(月).xls', names=['Date','increment from scaled industry'])  
df_industry_increment = df_industry_increment.iloc[119:-1,:].reset_index().drop(["index"], axis = 1)
df_industry_increment

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_industry_increment.shape[0]), 
         np.array(df_industry_increment.iloc[:,1]),label='increment from scaled industry')
plt.legend()
plt.show()

"""#### 宏观经济景气指数"""

df_mac_eco_index = pd.read_excel('宏观经济景气指数(月).xls', names=['Date','macro eco performance index'])  
df_mac_eco_index = df_mac_eco_index.iloc[106:,:].reset_index().drop(["index"], axis = 1)
df_mac_eco_index

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_mac_eco_index.shape[0]), 
         np.array(df_mac_eco_index.iloc[:,1]),label='macro eco performance index')
plt.legend()
plt.show()

"""#### 中国出口价格指数"""

df_CNY_import_index = pd.read_excel('出口价格指数HS2分类(月).xls', names=['Date','CNY HS2 import index'])  
df_CNY_import_index = df_CNY_import_index.iloc[81:,:].reset_index().drop(["index"], axis = 1)
df_CNY_import_index["Date"] = np.array(dates)
df_CNY_import_index

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_CNY_import_index.shape[0]), 
         np.array(df_CNY_import_index.iloc[:,1]),label='CNY HS2 import index')
plt.legend()
plt.show()

"""#### 人民币存款基准利率"""

df_CNY_saving_base = pd.read_excel('人民币存款基准利率.xls', names=['Date','CNY Saving Account Base Rate'])  
df_CNY_saving_base = df_CNY_saving_base.iloc[131:,:].reset_index().drop(["index"], axis = 1)
df_CNY_saving_base["Date"] = np.array(dates)
df_CNY_saving_base

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_CNY_saving_base.shape[0]), 
         np.array(df_CNY_saving_base.iloc[:,1]),label='China Saving Account Base Rate')
plt.legend()
plt.show()

"""#### 国家财政支出"""

df_fiscal = pd.read_excel('国家财政支出(月).xls', names=['Date','Government Fiscal Spending'])  
df_fiscal = df_fiscal.iloc[117:,:].reset_index().drop(["index"], axis = 1)
df_fiscal["Date"] = np.array(dates)
df_fiscal

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_fiscal.shape[0]), 
         np.array(df_fiscal.iloc[:,1]),label='Government Fiscal Spending')
plt.legend()
plt.show()

"""#### 官方外汇储备"""

df_CNY_foreign_res = pd.read_excel('官方外汇储备(月).xls', names=['Date','Official Foreign Asset Reserve'])  
df_CNY_foreign_res = df_CNY_foreign_res.iloc[126:-1,:].reset_index().drop(["index"], axis = 1)
df_CNY_foreign_res

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_CNY_foreign_res.shape[0]), 
         np.array(df_CNY_foreign_res.iloc[:,1]),label='Official Foreign Asset Reserve')
plt.legend()
plt.show()

"""#### 金融机构存款准备金率"""

df_deposit_reserve_ratio = pd.read_excel('金融机构存款准备金率.xls', names=['Date','Financial Institute Required Reserve Ratio'])  
df_deposit_reserve_ratio = df_deposit_reserve_ratio.iloc[179:-1,:].reset_index().drop(["index"], axis = 1)
df_deposit_reserve_ratio

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_deposit_reserve_ratio.shape[0]), 
         np.array(df_deposit_reserve_ratio.iloc[:,1]),
         label='Financial Institute Required Reserve Ratio')
plt.legend()
plt.show()

"""#### 消费者信心指数"""

df_consumer_confi_index = pd.read_excel('消费者信心指数.xls', names=['Date','Consumer Confidence Index'])  
df_consumer_confi_index = df_consumer_confi_index.iloc[107:,:].reset_index().drop(["index"], axis = 1)
df_consumer_confi_index

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_consumer_confi_index.shape[0]), 
         np.array(df_consumer_confi_index.iloc[:,1]),
         label='Consumer Confidence Index')
plt.legend()
plt.show()

"""#### Combine all date"""

df_China = pd.concat([df_M0,df_Chinese_CPI,df_oversea_inv,df_industry_increment,
                      df_mac_eco_index,df_CNY_import_index,df_CNY_saving_base,
                      df_fiscal,df_deposit_reserve_ratio,df_CNY_foreign_res,
                      df_consumer_confi_index],axis=1).drop(["Date"], axis =1) 
                      
df_China.insert(loc=0, column='Date', value=dates)
df_China = df_China.reset_index().drop(["index"], axis = 1)

df_China

"""### **Combine China and US data**"""

df_all = pd.concat([df_US, df_China], axis=1).drop(["Date"], axis =1)

df_all

"""### **Targets: USD/CNY Exchange Rate**"""

# read USD/CNY average rate
df_fx = pd.read_excel('USDCNY平均汇率(月).xls', names=['Date','USD/CNY'])  
df_fx = df_fx.iloc[119:-1,:]

# read the data
df_fx

plt.figure(figsize = (12,3))
plt.plot(df_fx['USD/CNY'])
plt.xlabel('date')
plt.ylabel('exchange rate')
plt.title('USD/CNY exchange rate')
plt.show()

# moving average calculation
df_fx['daily_dif'] = df_fx['USD/CNY'].diff(periods=1) # daily increase
df_fx['SMA_7_ON'] = df_fx['USD/CNY'].rolling(7, min_periods = 1).mean() # weekly simple moving average
df_fx['SMA_30_ON'] = df_fx['USD/CNY'].rolling(30, min_periods = 1).mean() # monthly simple moving average
df_fx['CMA'] = df_fx['USD/CNY'].expanding().mean() # cumulative moving average
df_fx['EMA_0.3'] = df_fx['USD/CNY'].ewm(alpha = 0.3, adjust = False).mean() # exponential moving average, alpha of 0.3
df_fx['EMA_0.1'] = df_fx['USD/CNY'].ewm(alpha = 0.1, adjust = False).mean() # exponential moving average, alpha of 0.1

df_fx = df_fx.reset_index().drop(["index"], axis=1)
df_fx

plt.figure(figsize = (12,3))

# number of plot
num_data = np.arange(0, df_fx.shape[0],step=1)

# plot for all kinds of moving average
plt.plot(num_data, df_fx['USD/CNY'],label='exchange rate')
plt.plot(num_data, df_fx['SMA_7_ON'],label='7 days MA')
plt.plot(num_data, df_fx['SMA_30_ON'],label='30 days MA')
plt.plot(num_data, df_fx['CMA'],label='EMA with alpha of 0.3')
plt.plot(num_data, df_fx['EMA_0.3'],label='EMA with alpha of 0.1')
plt.plot(num_data, df_fx['EMA_0.1'], label='CMA')
plt.xlabel('Days',size = 10)
plt.ylabel('Exchange rate',size = 10)
plt.legend()
plt.title('Different Moving Average',size = 15)
plt.show()

"""### **Final Dataset**"""

ds = pd.concat([df_fx[["USD/CNY"]], df_all], axis=1)

ds

# Splitting the dataset into train & test subsets
n_obs = 200
df_train, df_test = ds[:n_obs], ds[n_obs:]

"""### **Forecasting Model**

#### Check for stationality
"""

# Augmented Dickey-Fuller Test (ADF Test) to check for stationarity
def adf_test(ds):
    dftest = adfuller(ds, autolag='AIC')   # one that has minimum AIC among all the other models
    adf = pd.Series(dftest[0:4], index = ['Test Statistic','p-value','# Lags','# Observations'])

    for key, value in dftest[4].items():
       adf['Critical Value (%s)'%key] = value
    print (adf)

    p = adf['p-value']
    if p <= 0.05:
        print("\nSeries is Stationary")
    else:
        print("\nSeries is Non-Stationary")

"""#### First Order Differencing"""

for i in df_train.columns:
    print("Column: ",i)
    print('--------------------------------------')
    adf_test(df_train[i])
    print('\n')

# Differencing all variables to get rid of Stationarity
ds_differenced = df_train.diff().dropna()

"""#### Second Order Differencing"""

# Running the ADF test once again to test for Stationarity
for i in ds_differenced.columns:
    print("Column: ",i)
    print('--------------------------------------')
    adf_test(ds_differenced[i])
    print('\n')

# Some features are non-stationary
ds_differenced = ds_differenced.diff().dropna()

"""#### Third Order Differencing (not used to avoid over-differencing)"""

# Running the ADF test for the 3rd time to test for Stationarity
#for i in ds_differenced.columns:
#    print("Column: ",i)
#    print('--------------------------------------')
#    adf_test(ds_differenced[i])
#    print('\n')

"""**Three times differencing to achieve stationary for all the data. However, only 2 order of differencing is used to avoid over differencing.**

#### **VAR model**
"""

# Fitting the VAR model to the 2nd Differenced Data
model = VAR(ds_differenced)
results = model.fit(maxlags = 5, ic = 'aic')

results.summary()

# Forecasting for the rest 50 steps ahead
predicted = results.forecast(ds_differenced.values[:200], 50)
forecast = pd.DataFrame(predicted, index = ds.index[:50], columns = ds.columns)

# Inverting the Differencing Transformation
def invert_transformation(ds, df_forecast, second_diff=False):
    for col in ds.columns:
        # Undo the 2nd Differencing
        if second_diff:
            df_forecast[str(col)] = (ds[col].iloc[-1] - ds[col].iloc[-2]) + df_forecast[str(col)].cumsum()

        # Undo the 1st Differencing
        df_forecast[str(col)] = ds[col].iloc[-1] + df_forecast[str(col)].cumsum()

    return df_forecast

forecast_values = invert_transformation(df_train, forecast, second_diff=True)

# Actual vs Forecasted Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(50), np.array(forecast_values["USD/CNY"]),label='forecast')
plt.plot(np.arange(50), np.array(ds["USD/CNY"].iloc[-50:]),label='actual')
plt.legend()
plt.show()

y_valid = np.array(forecast_values["USD/CNY"])
preds = np.array(ds["USD/CNY"].iloc[-50:])
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# feature importance
X = ds.iloc[:,1:]
y = ds.iloc[:,0]
X_std = MinMaxScaler().fit_transform(X) # scaling based on range for different features 
 
clf = RandomForestRegressor(random_state = 0, n_jobs = -1)
predictor = clf.fit(X_std, y)
feature_importance = predictor.feature_importances_ # feature importance

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""#### **Decison Trees**"""

x_train, y_train = np.array(df_train.iloc[:,1:]), np.array(df_train.iloc[:,0])
x_test, y_test = np.array(df_test.iloc[:,1:]), np.array(df_test.iloc[:,0])

DS = DecisionTreeRegressor()
criterion = ['mse','mae',"poisson"]
max_depth = [25,50,100,200,1000]
splitter = ['best', 'random']
param_grid = dict(criterion = criterion, max_depth = max_depth, splitter = splitter)
grid = GridSearchCV(estimator = DS, param_grid = param_grid, scoring = 'neg_mean_absolute_error', verbose = 0, n_jobs = -1, cv = 5)

result = grid.fit(x_train,y_train)
print('best score:', result.best_score_)
print('best parameters:', result.best_params_)

final_DS = DecisionTreeRegressor(criterion = "mse", max_depth = 100, splitter='random').fit(x_train,y_train)
DS_pre = final_DS.predict(x_test)

y_valid = y_test
preds = final_DS.predict(x_test)
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# Decision Tree Forecasted vs Acutal Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(50), DS_pre, label='DS forecast')
plt.plot(np.arange(50), y_test,label='actual')
plt.legend()
plt.show()

X = ds.iloc[:,1:]
y = ds.iloc[:,0]

#scaling based on range for different features 
X_std = MinMaxScaler().fit_transform(X) 
 
clf = DecisionTreeRegressor(criterion = "mse", max_depth = 25, splitter='random')
predictor = clf.fit(X_std, y)

# feature importance
feature_importance = predictor.feature_importances_

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""#### **Random Forest**"""

x_train, y_train = np.array(df_train.iloc[:,1:]), np.array(df_train.iloc[:,0])
x_test, y_test = np.array(df_test.iloc[:,1:]), np.array(df_test.iloc[:,0])

RS = RandomForestRegressor()
n_estimators = [10,50,100,200]
max_depth = [50,100,200,500,1000]
criterion = ['mse','mae']
param_grid = dict(n_estimators = n_estimators, max_depth = max_depth,criterion = criterion)
grid = GridSearchCV(estimator = RS, param_grid = param_grid, scoring = 'neg_mean_absolute_error', verbose = 0, n_jobs = -1, cv = 5)

#result = grid.fit(x_train,y_train)
#print('best score:', result.best_score_)
#print('best parameters:', result.best_params_)

final_RF = RandomForestRegressor(criterion = "mse", max_depth = 200, n_estimators = 10).fit(x_train,y_train)

y_valid = y_test
preds = final_RF.predict(x_test)
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# Random Forest Forecasted vs Acutal Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(50), preds, label='RF forecast')
plt.plot(np.arange(50), y_test,label='actual')
plt.legend()
plt.show()

# feature importance
X = df_train.iloc[:,1:]
y = df_train.iloc[:,0]

#scaling based on range for different features 
X_std = MinMaxScaler().fit_transform(X) 
 
clf = final_RF
predictor = clf.fit(X_std, y)

# feature importance
feature_importance = predictor.feature_importances_

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""#### **Xgboost**"""

# set up all the parameters
n_estimators = [10,50,200,300] 
learning_rate = [0.1,0.5,1]  
max_depth = [50,100,200]  
min_child_weight = [4,6,10]
subsample = [0.1,0.9,0.3,0.7] 
colsample_bytree = [ 0.2,0.6,0.8] 
gamma = [0.1,0.2]
reg_alpha = [0.1,0.01,0.05,1,4]  
reg_lambda = [0.01,0.05,1,0.5,1,4] 

param_grid = dict(n_estimators=n_estimators, learning_rate=learning_rate,
                  max_depth=max_depth, min_child_weight=min_child_weight,
                  reg_alpha=reg_alpha,subsample=subsample,gamma=gamma,
                  colsample_bytree=colsample_bytree,reg_lambda=reg_lambda)

XGB = XGBRegressor()
grid = GridSearchCV(estimator = XGB, param_grid = param_grid, 
                    scoring = 'neg_mean_absolute_error', verbose = 0, n_jobs = -1, cv = 5)

#result = grid.fit(x_train,y_train)
#print('best score:', result.best_score_)
#print('best parameters:', result.best_params_)

final_xgb = XGBRegressor(n_estimators=50, learning_rate=0.5, max_depth=50, 
                         min_child_weight=8,gamma=0.1,reg_alpha=1, reg_lambda=1,
                         colsample_bytree= 0.2, subsample=0.3).fit(x_train,y_train)                      
xgb_pre = final_xgb.predict(x_test)

y_valid = y_test
preds = xgb_pre
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# Random Forest Forecasted vs Acutal Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(50), xgb_pre, label='xgb forecast')
plt.plot(np.arange(50), y_test,label='actual')
plt.legend()
plt.show()

# feature importance
X = df_train.iloc[:,1:]
y = df_train.iloc[:,0]

#scaling based on range for different features 
X_std = MinMaxScaler().fit_transform(X) 
 
clf = final_xgb
predictor = clf.fit(X_std, y)

# feature importance
feature_importance = predictor.feature_importances_

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""## **Micro Factors**

#### 1年中债国债到期收益率
"""

df_CNY_1_yr_bond = pd.read_excel('1yr中债国债到期收益率(中债)(日).xls', names=['Date','1_yr_AAA_CNY_govern_bond'])  
df_CNY_1_yr_bond = df_CNY_1_yr_bond.iloc[1995:,:].reset_index().drop(["index"], axis = 1)
df_CNY_1_yr_bond

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_CNY_1_yr_bond.shape[0]), np.array(df_CNY_1_yr_bond.iloc[:,1]),
         label='1_yr_AAA_CNY_govern_bond')
plt.legend()
plt.show()

"""#### NASDAQ index"""

df_NASDAQ = pd.read_excel('NASDAQ(日).xls', names=['Date','NASDAQ'])  
df_NASDAQ = df_NASDAQ.iloc[9817:,:].reset_index().drop(["index"], axis = 1)
df_NASDAQ

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_NASDAQ.shape[0]), np.array(df_NASDAQ.iloc[:,1]),label='NASDAQ')
plt.legend()
plt.show()

"""#### **QDII 投资额度**"""

#df_QDII_scale = pd.read_excel('QDII投资额度.xls', names=['Date','QDII_scale'])  
#df_QDII_scale = df_QDII_scale.iloc[14:,:].reset_index().drop(["index"], axis = 1)
#df_QDII_scale

#plt.figure(figsize = (12,3))
#plt.plot(np.arange(df_QDII_scale.shape[0]), np.array(df_QDII_scale.iloc[:,1]),label='QDII scale')
#plt.legend()
#plt.show()

"""#### SP500主要指数(日)"""

df_sp500 = pd.read_excel('sp500主要指数(日).xls', names=['Date','sp500'])  
df_sp500 = df_sp500.iloc[20598:,:].reset_index().drop(["index"], axis = 1)
df_sp500

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_sp500.shape[0]), np.array(df_sp500.iloc[:,1]),label='sp 500 index')
plt.legend()
plt.show()

"""#### USD Libor"""

df_usd_libor = pd.read_excel('USD伦敦同业拆借利率(LIBOR)(日).xls', names=['Date','USD_LIBOR'])  
df_usd_libor = df_usd_libor.iloc[2214:,:].reset_index().drop(["index"], axis = 1)
df_usd_libor

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_usd_libor.shape[0]), np.array(df_usd_libor.iloc[:,1]),label='USD Libor')
plt.legend()
plt.show()

"""#### SHIBOR rate"""

df_shibor = pd.read_excel('上海银行间同业拆放利率(SHIBOR)(日).xls', names=['Date','Shibor'])  
df_shibor = df_shibor.iloc[14:,:].reset_index().drop(["index"], axis = 1)
df_shibor

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_shibor.shape[0]), np.array(df_shibor.iloc[:,1]),label='Shibor Rate')
plt.legend()
plt.show()

"""#### 中债国开债到期收益率(中债)(日)"""

df_CDB_bond = pd.read_excel('中债国开债到期收益率(中债)(日).xls', names=['Date','1_yr_CDB'])  
df_CDB_bond = df_CDB_bond.iloc[1991:,:].reset_index().drop(["index"], axis = 1)
df_CDB_bond

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_CDB_bond.shape[0]), np.array(df_CDB_bond.iloc[:,1]),label='1_yr_CDB_Bond')
plt.legend()
plt.show()

"""#### USD/CNY 即期汇率（1D）(not used)"""

#df_USDCNY_spot = pd.read_excel('USDCNY即期汇率(日).xls', names=['Date','USD/CNY_spot'])  
#df_USDCNY_spot = df_USDCNY_spot.iloc[7429:,:].reset_index().drop(["index"], axis = 1)
#df_USDCNY_spot

#plt.figure(figsize = (12,3))
#plt.plot(np.arange(df_USDCNY_spot.shape[0]), np.array(df_USDCNY_spot.iloc[:,1]),label='USD/CNY spot')
#plt.legend()
#plt.show()

"""#### **USD/CNY远期汇率（1D**）(not used)"""

#df_USDCNY_foward = pd.read_excel('外汇远期（1D）成交统计.xls', names=['Date','USDCNY_future'])  
#df_USDCNY_foward = df_USDCNY_foward.iloc[230:-1,:].reset_index().drop(["index"], axis = 1)
#df_USDCNY_foward

#plt.figure(figsize = (12,3))
#plt.plot(np.arange(df_USDCNY_future.shape[0]), np.array(df_USDCNY_future.iloc[:,1]),label='USDCNY future')
#plt.legend()
#plt.show()

"""#### SP500 VIX"""

df_sp500_vix = pd.read_excel('市场波动率指数(VIX).xls', names=['Date','sp500_vix'])  
df_sp500_vix = df_sp500_vix.iloc[5041:,:].reset_index().drop(["index"], axis = 1)
df_sp500_vix

plt.figure(figsize = (12,3))
plt.plot(np.arange(df_sp500_vix.shape[0]), np.array(df_sp500_vix.iloc[:,1]),label='sp500_vix')
plt.legend()
plt.show()

"""#### USDCNY daily fx"""

df_daily_fx = pd.read_excel('USDCNY人民币汇率(日).xls', names=['Date','USD/CNY_daily_fx'])  
df_daily_fx = df_daily_fx.iloc[3797:,:].reset_index().drop(["index"], axis = 1)
df_daily_fx

"""#### Combine all micro data"""

df_micro = df_daily_fx.set_index('Date').join(df_CNY_1_yr_bond.set_index('Date')).join(
    df_NASDAQ.set_index('Date')).join(df_sp500.set_index('Date')).join(
        df_usd_libor.set_index('Date')).join(df_shibor.set_index('Date')).join(
            df_CDB_bond.set_index('Date')).join(df_sp500_vix.set_index('Date'))   #join(df_USDCNY_spot.set_index('Date'))

df_2 = df_micro
df_micro = df_micro.dropna().reset_index().drop(["Date"],axis = 1)

df_micro

# Splitting the dataset into train & test subsets
n_obs = 2000
df_mic_train, df_mic_test = df_micro[:n_obs], df_micro[n_obs:]

"""### **Forecasting Model**

#### Check for stationality
"""

# Augmented Dickey-Fuller Test (ADF Test) to check for stationarity
def adf_test(ds):
    dftest = adfuller(ds, autolag='AIC')   # one that has minimum AIC among all the other models
    adf = pd.Series(dftest[0:4], index = ['Test Statistic','p-value','# Lags','# Observations'])

    for key, value in dftest[4].items():
       adf['Critical Value (%s)'%key] = value
    print (adf)

    p = adf['p-value']
    if p <= 0.05:
        print("\nSeries is Stationary")
    else:
        print("\nSeries is Non-Stationary")

"""#### First Order Differencing"""

for i in df_micro.columns:
    print("Column: ",i)
    print('--------------------------------------')
    adf_test(df_micro[i])
    print('\n')

# Differencing all variables to get rid of Stationarity
ds_differenced = df_micro.diff().dropna()

"""#### Second Order Differencing"""

# Running the ADF test once again to test for Stationarity
for i in ds_differenced.columns:
    print("Column: ",i)
    print('--------------------------------------')
    adf_test(ds_differenced[i])
    print('\n')

# Some features are non-stationary
ds_differenced = ds_differenced.diff().dropna()

"""#### **VAR model**"""

# Fitting the VAR model to the 2nd Differenced Data
model = VAR(ds_differenced)
results = model.fit(maxlags = 5, ic = 'aic')

results.summary()

# Forecasting for the rest 50 steps ahead
num_forcast = ds_differenced.shape[0] - 2000
predicted = results.forecast(ds_differenced.values[:2000], num_forcast)
forecast = pd.DataFrame(predicted, index = df_micro.index[:num_forcast], columns = df_micro.columns)

# Inverting the Differencing Transformation
def invert_transformation(ds, df_forecast, second_diff=False):
    for col in ds.columns:
        # Undo the 2nd Differencing
        if second_diff:
            df_forecast[str(col)] = (ds[col].iloc[-1] - ds[col].iloc[-2]) + df_forecast[str(col)].cumsum()

        # Undo the 1st Differencing
        df_forecast[str(col)] = ds[col].iloc[-1] + df_forecast[str(col)].cumsum()

    return df_forecast

forecast_values = invert_transformation(df_mic_train, forecast, second_diff=True)

forecast_values.head()

# Actual vs Forecasted Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(num_forcast), np.array(forecast_values["USD/CNY_daily_fx"]),label='forecast')
plt.plot(np.arange(num_forcast), np.array(df_mic_train["USD/CNY_daily_fx"].iloc[-num_forcast:]),label='actual')
plt.legend()
plt.show()

y_valid = np.array(df_mic_train["USD/CNY_daily_fx"].iloc[-num_forcast:])
preds = np.array(forecast_values["USD/CNY_daily_fx"])
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# feature importance
X = df_mic_train.iloc[:,1:]
y = df_mic_train.iloc[:,0]
X_std = MinMaxScaler().fit_transform(X) # scaling based on range for different features 
 
clf = RandomForestRegressor(random_state = 0, n_jobs = -1)
predictor = clf.fit(X_std, y)
feature_importance = predictor.feature_importances_ # feature importance

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""#### **Decison Trees**"""

x_train, y_train = np.array(df_mic_train.iloc[:,1:]), np.array(df_mic_train.iloc[:,0])
x_test, y_test = np.array(df_mic_test.iloc[:,1:]), np.array(df_mic_test.iloc[:,0])

DS = DecisionTreeRegressor()
criterion = ['mse','mae',"poisson"]
max_depth = [25,50,100,200,1000]
splitter = ['best', 'random']
param_grid = dict(criterion = criterion, max_depth = max_depth, splitter = splitter)
grid = GridSearchCV(estimator = DS, param_grid = param_grid, scoring = 'neg_mean_absolute_error', verbose = 0, n_jobs = -1, cv = 5)

#result = grid.fit(x_train,y_train)
#print('best score:', result.best_score_)
#print('best parameters:', result.best_params_)

final_DS = DecisionTreeRegressor(criterion = "mae", max_depth = 25, splitter='best').fit(x_train,y_train)
DS_pre = final_DS.predict(x_test)

y_valid = y_test
preds = DS_pre
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# Decision Tree Forecasted vs Acutal Plots
num_forcast = df_micro.shape[0] - 2000
plt.figure(figsize = (20,6))
plt.plot(np.arange(num_forcast), DS_pre, label='DS forecast')
plt.plot(np.arange(num_forcast), y_test,label='actual')
plt.legend()
plt.show()

X = df_micro.iloc[:,1:]
y = df_micro.iloc[:,0]

#scaling based on range for different features 
X_std = MinMaxScaler().fit_transform(X) 
 
clf = DecisionTreeRegressor(criterion = "mae", max_depth = 25, splitter='random')
predictor = clf.fit(X_std, y)

# feature importance
feature_importance = predictor.feature_importances_

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""#### **Random Forest**"""

x_train, y_train = np.array(df_mic_train.iloc[:,1:]), np.array(df_mic_train.iloc[:,0])
x_test, y_test = np.array(df_mic_test.iloc[:,1:]), np.array(df_mic_test.iloc[:,0])

RS = RandomForestRegressor()
n_estimators = [10,50,100,200]
max_depth = [50,100,200,500,1000]
criterion = ['mse','mae']
param_grid = dict(n_estimators = n_estimators, max_depth = max_depth,criterion = criterion)
grid = GridSearchCV(estimator = RS, param_grid = param_grid, scoring = 'neg_mean_absolute_error', verbose = 0, n_jobs = -1, cv = 5)

#result = grid.fit(x_train,y_train)
#print('best score:', result.best_score_)
#print('best parameters:', result.best_params_)

final_RF = RandomForestRegressor(criterion = "mae", max_depth = 50, n_estimators = 50).fit(x_train,y_train)
RF_pre = final_RF.predict(x_test)

y_valid = y_test
preds = RF_pre
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# Random Forest Forecasted vs Acutal Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(RF_pre.shape[0]),RF_pre,label='RF forecast')
plt.plot(np.arange(RF_pre.shape[0]),y_test,label='actual')
plt.legend()
plt.show()

# feature importance
X = df_mic_train.iloc[:,1:]
y = df_mic_train.iloc[:,0]

#scaling based on range for different features 
X_std = MinMaxScaler().fit_transform(X) 
 
clf = final_RF
predictor = clf.fit(X_std, y)

# feature importance
feature_importance = predictor.feature_importances_

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""#### **Xgboost**"""

# set up all the parameters
n_estimators = [10,50,200,300] 
learning_rate = [0.1,0.2,0.5,1] 
max_depth = [50,100,200]
min_child_weight = [4,6,10]
subsample = [0.1,0.3,0.7,0.9] 
colsample_bytree = [0.2,0.6,0.8] 
gamma = [0.1,0.2]
reg_alpha = [0.01,0.05,0.1,1,4] 
reg_lambda = [0.01,0.05,0.5,1,4]

param_grid = dict(n_estimators=n_estimators, learning_rate=learning_rate,
                  max_depth=max_depth, min_child_weight=min_child_weight,
                  reg_alpha=reg_alpha,subsample=subsample,gamma=gamma,
                  colsample_bytree=colsample_bytree,reg_lambda=reg_lambda)

XGB = XGBRegressor()
grid = GridSearchCV(estimator = XGB, param_grid = param_grid, scoring = 'neg_mean_absolute_error', verbose = 0, n_jobs = -1, cv = 5)

#result = grid.fit(x_train,y_train)
#print('best score:', result.best_score_)
#print('best parameters:', result.best_params_)

final_xgb = XGBRegressor(n_estimators=300, learning_rate=0.2, max_depth=100, 
                         min_child_weight=10,gamma=0.2,reg_alpha=0.1, reg_lambda=1,
                         colsample_bytree= 0.2, subsample=0.3).fit(x_train,y_train) 

xgb_pre = final_xgb.predict(x_test)

y_valid = y_test
preds = xgb_pre
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")

# XGB Forecasted vs Acutal Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(xgb_pre.shape[0]), xgb_pre,label='xgb forecast')
plt.plot(np.arange(xgb_pre.shape[0]), y_test,label='actual')
plt.legend()
plt.show()

# feature importance
X = df_mic_train.iloc[:,1:]
y = df_mic_train.iloc[:,0]

#scaling based on range for different features 
X_std = MinMaxScaler().fit_transform(X) 
 
clf = final_xgb
predictor = clf.fit(X_std, y)

# feature importance
feature_importance = predictor.feature_importances_

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])

"""## **Fianl Model: major components:Chinese per GDP, Net Foreign Assets and Term of Trade**

### per GDP
"""

# extract data for per GDP data
df_per_GDP = pd.read_excel('Per GDP.xls', names=['Date','per gdp in RMB'])
df_per_GDP = df_per_GDP.iloc[2:-1,:].reset_index().drop(["index"],axis=1).drop(["Date"],axis = 1)
df_per_GDP

"""### net foreign asset"""

# extract data for net foreign asset
df_current_account = pd.read_excel('经常账户差额当季值(亿美元).xls', names=['Date','net foreign asset in USD']) 
df_current_account = df_current_account.iloc[10:,:].reset_index().drop(["index"],axis=1)
df_current_account
#df_BOA = pd.read_excel('国际收支总差额累计值(亿美元).xls', names=['Date','国际总收支（亿美元）'])

"""### avg fx rate"""

# avg fx rate
df_fx_2 = pd.read_excel('real_monthly_fx.xls',names=['Date',"real time fx","CNY cpi","US CPI",'real fx'])  
df_real_fx = df_fx_2.iloc[130:-5]
df_real_fx['season_real_fx'] = df_real_fx['real fx'].rolling(3, min_periods = 1).mean() 
df_real_fx = df_real_fx.iloc[::3,:]
df_real_fx = df_real_fx[['season_real_fx']].reset_index().drop(["index"],axis=1)
df_real_fx

"""### term of trade"""

# extract data for net foreign asset
df_tot = pd.read_excel('term_of_trade.xls', names=['Date','import','export','term of trade']) 
df_tot = df_tot.iloc[75:-3,:].reset_index().drop(["index"],axis=1)
df_tot['season_tot'] = df_tot['term of trade'].rolling(3, min_periods = 1).mean() 
df_tot = df_tot.iloc[::3,:]
df_tot = df_tot[["Date",'season_tot']].reset_index().drop(["index"],axis=1).drop(["Date"],axis = 1)
df_tot

"""### Finalize all data"""

# merge all tables 
df_data = pd.concat([df_real_fx,df_current_account,df_per_GDP,df_tot],axis=1)
df_data

# re-organize all the data on monthly basis
df_1 = pd.concat([df_US, df_China], axis=1).iloc[11:-4]
all_date = df_1.iloc[:,0]
df_1 = df_1.drop(["Date"],axis = 1)
df_1.insert(0,'Date',value=all_date)
df_1 = df_1.rolling(3, min_periods = 1).mean() 
df_1 = df_1.iloc[::3,:].reset_index().drop(["index"],axis=1)
df_1

# select useful index from previous predictions
df_selected = df_1[["Official Foreign Asset Reserve","M0","US_gov_deficit",
                    "US_gov_assets","Financial Institute Required Reserve Ratio",
                    "US_money_supply","US_Bond_return"]]

# merge two tables
df_3 = pd.concat([df_data, df_selected],axis = 1).drop(["Date"],axis=1).iloc[:-1]
df_3

"""### Apply selected data to predict exchange rate"""

x_train, y_train = np.array(df_3.iloc[:50,1:]), np.array(df_3.iloc[:50,0])
x_test, y_test = np.array(df_3.iloc[50:,1:]), np.array(df_3.iloc[50:,0])

# set up all the parameters
n_estimators = [10,50,200,300] 
learning_rate = [0.1,0.2,0.5,1] 
max_depth = [50,100,200]
min_child_weight = [4,6,10]
subsample = [0.1,0.3,0.7,0.9] 
colsample_bytree = [0.2,0.6,0.8] 
gamma = [0.1,0.2]
reg_alpha = [0.01,0.05,0.1,1,4] 
reg_lambda = [0.01,0.05,0.5,1,4]

param_grid = dict(n_estimators=n_estimators, learning_rate=learning_rate,
                  max_depth=max_depth, min_child_weight=min_child_weight,
                  reg_alpha=reg_alpha,subsample=subsample,gamma=gamma,
                  colsample_bytree=colsample_bytree,reg_lambda=reg_lambda)

XGB = XGBRegressor()
grid = GridSearchCV(estimator = XGB, param_grid = param_grid, scoring = 'neg_mean_absolute_error', verbose = 0, n_jobs = -1, cv = 5)

#result = grid.fit(x_train,y_train)
#print('best score:', result.best_score_)
#print('best parameters:', result.best_params_)

final_xgb = XGBRegressor(n_estimators=10, learning_rate=0.2, max_depth=100, 
                         min_child_weight= 4,gamma=0.1,reg_alpha = 4, reg_lambda = 0.01,
                         colsample_bytree= 0.6, subsample = 0.9).fit(x_train,y_train) 

xgb_pre = final_xgb.predict(x_test)

y_valid = y_test
preds = xgb_pre
SMAPE = (100/len(y_valid)) * np.sum(2 * np.abs(preds - y_valid) / (np.abs(y_valid) + np.abs(preds)))
final_RMSE = mean_squared_error(y_valid, preds, squared=False)

print("the symmetric mean absolute percentage error is", round(SMAPE,3),"%")
print("the RMSE is", round(final_RMSE,3))

# Random Forest Forecasted vs Acutal Plots
plt.figure(figsize = (12,3))
plt.plot(np.arange(xgb_pre.shape[0]), xgb_pre,label='xgb forecast')
plt.plot(np.arange(xgb_pre.shape[0]), y_test,label='actual')
plt.legend()
plt.show()

# feature importance
X = df_3.iloc[:,1:]
y = df_3.iloc[:,0]

#scaling based on range for different features 
X_std = MinMaxScaler().fit_transform(X) 
 
clf = final_xgb
predictor = clf.fit(X_std, y)

# feature importance
feature_importance = predictor.feature_importances_

Feature_name = X.columns
indices = feature_importance.argsort()[::-1][0:30]
feature_importance [[indices]]
Feature_name[[indices]]

plt.figure(figsize=(10,8))
plt.title('Top 30 Important Features',fontsize = 15)
plt.ylabel('feature importance',fontsize = 15)
plt.xticks(rotation = 90)
plt.bar(Feature_name[[indices]],feature_importance[[indices]])